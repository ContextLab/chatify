from typing import Any, Dict, List, Optional

import requests


from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, LLMMathChain
from langchain.chains.base import Chain


from typing import Any, Dict, Optional
from langchain.callbacks.manager import CallbackManagerForChainRun

from .llm_models import ModelsFactory
from .cache import LLMCacher

from .utils import compress_code


class RequestChain(Chain):
    llm_chain: LLMChain = None
    prompt: Optional[Dict[str, Any]]
    headers: Optional[Dict[str, str]] = {
        'accept': 'application/json',
        'Content-Type': 'application/json',
    }
    input_key: str = 'text'
    url: str = "url"  #: :meta private:
    output_key: str = "text"  #: :meta private:

    @property
    def input_keys(self) -> List[str]:
        """Will be whatever keys the prompt expects.

        :meta private:
        """
        return [self.input_key]

    @property
    def output_keys(self) -> List[str]:
        """Will always return text key.

        :meta private:
        """
        return [self.output_key]

    def _call(
        self,
        inputs: Dict[str, Any],
        run_manager: Optional[CallbackManagerForChainRun] = None,
    ) -> Dict[str, Any]:
        # Prepare data
        if self.url != '/':
            self.url += '/'
        combined_url = self.url + self.prompt['prompt_id'] + '/response'
        data = {'user_text': inputs[self.input_key]}

        # Send the request
        response = requests.post(url=combined_url, headers=self.headers, json=data)
        output = eval(response.content.decode('utf-8'))

        return {self.output_key: output}


class CreateLLMChain:
    """Class for creating and executing Language Model (LLM) chains."""

    def __init__(self, config):
        """Initializes the CreateLLMChain object.

        Parameters
        ----------
        config (dict): Configuration settings for the LLM chain.

        Returns
        -------
        None
        """
        self.config = config
        self.chain_config = config['chain_config']

        self.llm_model = None
        self.llm_models_factory = ModelsFactory()

        self.cache = config['cache_config']['cache']
        self.cacher = LLMCacher(config)

        # Setup model and chain factory
        self._setup_llm_model(config['model_config'])
        self._setup_chain_factory()

        return None

    def _setup_llm_model(self, model_config):
        """Sets up the LLM model.

        Returns
        --------
        None
        """
        if self.llm_model is None:
            self.llm_model = self.llm_models_factory.get_model(model_config)

            if self.cache:
                self.llm_model = self.cacher.cache_llm(self.llm_model)

    def _setup_chain_factory(self):
        """Sets up the chain factory dictionary.

        Returns
        --------
        None
        """
        self.chain_factory = {
            'math': LLMMathChain,
            'default': LLMChain,
            'proxy': RequestChain,
        }

    def create_prompt(self, prompt):
        """Creates a prompt using a template and input variables.

        Parameters
        ----------
        prompt (dict): Prompt configuration containing the template and input variables.

        Returns
        -------
        PROMPT (PromptTemplate): Prompt template object.
        """
        PROMPT = PromptTemplate(
            template=prompt['content'], input_variables=prompt['input_variables']
        )
        return PROMPT

    def create_chain(self, model_config=None, prompt_template=None):
        """Creates an LLM chain based on the model configuration and prompt template.

        Parameters
        ----------
        model_config (dict): Configuration settings for the LLM model.
        prompt_template (PromptTemplate): Prompt template object.

        Returns
        -------
        chain (LLMChain): LLM chain object.
        """
        if self.config['chain_config']['chain_type'] == 'proxy':
            chain = RequestChain(
                url=self.config['model_config']['proxy_url'], prompt=prompt_template
            )
        else:
            try:
                chain_type = self.chain_config['chain_type']
            except KeyError:
                chain_type = 'default'

            chain = self.chain_factory[chain_type](
                llm=self.llm_model, prompt=self.create_prompt(prompt_template)
            )
        return chain

    def execute(self, chain, inputs, *args, **kwargs):
        """Executes the LLM chain with the given inputs.

        Parameters
        ----------
        chain (LLMChain): LLM chain object.
        inputs (str): Input text to be processed by the chain.
        *args: Additional positional arguments.
        **kwargs: Additional keyword arguments.

        Returns
        -------
        output: Output text generated by the LLM chain.
        """
        if self.cache:
            inputs = chain.prompt.format(text=compress_code(inputs))
            output = chain.llm(inputs, cache_obj=self.cacher.llm_cache)
            self.cacher.llm_cache.flush()
        else:
            output = chain(inputs)['text']

        return output
